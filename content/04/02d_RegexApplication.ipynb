{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finding words near each other\n",
    "\n",
    "Congrats on surviving the last few pages on regex. I'm guessing either you're asleep, giving up on regex, or you are struggling to keep your eyes from glazing over. \n",
    "\n",
    "[That's natural! No one really likes or understands regex. I mean, the memes are good... and vicious.](https://www.google.com/search?&tbm=isch&q=regex+memes)\n",
    "\n",
    "So three things are going on:\n",
    "1. We're going to introduce a new function - `NEAR_regex()` - that will let use leverage the power of regex while _moooostly_ hiding us from the necessary work of writing a dang regex.\n",
    "2. The prior two pages give you some background that will help you understand how to use this function.\n",
    "3. Again, this function means you won't need to write a regex pattern![^best]\n",
    "\n",
    "[^best]: ![best](https://memegenerator.net/img/instances/82084173/oh-my-gosh-youre-the-best.jpg)\n",
    "\n",
    "The [community codebook](https://github.com/LeDataSciFi/ledatascifi-2022/tree/main/community_codebook) (a valuable resource!) has a file in it called \"near_regex.py\". To get that file to the folder you're working on that needs the function (e.g., your class notes folder, and the assignment) you have two options:\n",
    "1. If you already cloned the textbook's repo (good job, you!), then the file is already on your computer. Simply find it inside `ledatascifi-2022/community_codebook/` and copy it, then paste to the folder where it is needed.\n",
    "1. [Clicking on the filename in the above link, then the \"Raw\" button will get you to this page](https://raw.githubusercontent.com/LeDataSciFi/ledatascifi-2022/main/community_codebook/near_regex.py). If you right click on the page and then select \"Save Page As\" (or similar, depending on browser), you can put it in the folder where it is needed.\n",
    "\n",
    "After you copy the function to the same location as your code, load it into your code by adding this to the top of your code:\n",
    "\n",
    "```python\n",
    "from near_regex import NEAR_regex # yes, the font case is different\n",
    "```\n",
    "\n",
    "Oh, and the near_regex.py file has examples in it, which you don't need. Delete everything below the \"return\" line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "Let me start by showing you some examples. After these example, load the function and read the help documentation in it, and then we can do some practice.\n",
    "\n",
    "Each example looks inside a document (here, the \"document\" is just a short sentence) for some words.\n",
    "\n",
    "To use this function:\n",
    "1. Load your document and clean the string \n",
    "2. Create a list of strings you want to look for. The function will create a (complex) regex pattern that will detect if all of the strings are near each other in the document. \n",
    "3. Give the list of strings (from step 2) to the function to create the regex pattern.\n",
    "4. Use the pattern (from step 3) to\n",
    "    - Count how many times the pattern hits\n",
    "    - Print out the text matches to check the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def NEAR_regex(list_of_words,max_words_between=5,partial=False,cases_matter=False):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_words : list\n",
    "        A list of \"words\", each element is a string\n",
    "        \n",
    "        This program will return a regex that will look for times where word1 \n",
    "        is near word2, or word2 is near word 1.\n",
    "        \n",
    "        It works with multiple words: You can see if words1 is near word2 or\n",
    "        word3. \n",
    "        \n",
    "    max_words_between : int, optional\n",
    "        How many \"words\" are allowed between words in list_of_words. The default\n",
    "        is 5, but you should consider this carefully.\n",
    "        \n",
    "        \"words\" in between are chunks of characters. \"DON don don- don12 2454\" \n",
    "        is 5 words.\n",
    "        \n",
    "        This will not allow matches if the words are separated by a newline \n",
    "        (\"\\n\") character.\n",
    "        \n",
    "    partial : Boolean, optional\n",
    "        If true, will accept longer words than you give. For example, if one \n",
    "        word in your list is \"how\", it will match to \"howdy\". Be careful in \n",
    "        choosing this based on your problem. Partial makes more sense with \n",
    "        longer words. \n",
    "        The default is True.\n",
    "        \n",
    "    cases_matter: Boolean, optional bt IMPORTANT\n",
    "        If True, will return a regex string that will only catch cases where  \n",
    "        words in the string have the same case as given as input to this \n",
    "        function. For example, if one word here is \"Hi\", then the regex \n",
    "        produced by this function will not catch \"hi\".\n",
    "        \n",
    "        If false, will return a regex string that will only work if all letters\n",
    "        in search string are lowercase.\n",
    "        \n",
    "        The default is True.\n",
    "     \n",
    "    Warning\n",
    "    -------\n",
    "    See the last example. The regex is minimally greedy. \n",
    "    \n",
    "       \n",
    "    Feature Requests (participation credit available)\n",
    "    -------\n",
    "    1. A wrapper that takes the above inputs, the string to search, and a variable=count|text, and\n",
    "    returns either the number of hits or the text of the matching elements. (See the examples below.)\n",
    "    \n",
    "    2. Optionally clean the string before the regex stuff happens.\n",
    "    \n",
    "    3. Optionally ignore line breaks. \n",
    "    \n",
    "    4. Optionally make it lazy (in the last example, \n",
    "    the \"correct\" answer is probably 2, but it gives 1.)\n",
    "    \n",
    "        \n",
    "    Unsure about speed\n",
    "    -------\n",
    "    I don't think this is a very \"fast\" function, but it should be robust. \n",
    "  \n",
    "    \n",
    "    Suggested use\n",
    "    -------\n",
    "    # clean your starting string \n",
    "    a_string_you_have = 'jack and jill went up the hill'\n",
    "    \n",
    "    # 1. define words and set up the regex\n",
    "    words = ['jack','hill']                         \n",
    "    rgx = NEAR_regex(words)                       \n",
    "    \n",
    "    # 2a. count the number of times the word groups are in the text near each other\n",
    "    count = len(re.findall(rgx,a_string_you_have))              \n",
    "    print(count)  \n",
    "    \n",
    "    # 2b. print the actual text matches <-- great for checking!\n",
    "    text_of_matches = [m.group(0) for m in re.finditer(rgx,a_string_you_have)]\n",
    "    print(text_of_matches)\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A string which is a regex that can be used to look for cases where all the \n",
    "    input words are near each other.\n",
    "\n",
    "    '''\n",
    "               \n",
    "    from itertools import permutations\n",
    "    \n",
    "    start = r'(?:\\b' # the r means \"raw\" as in the backslash is just a backslash, not an escape character\n",
    "    \n",
    "    if partial:\n",
    "        gap   = r'[A-Za-z]*\\b(?: +[^ \\n\\r]*){0,' +str(max_words_between)+r'} *\\b'\n",
    "        end   = r'[A-Za-z]*\\b)'\n",
    "    else:\n",
    "        gap   = r'\\b(?: +[^ \\n]*){0,' +str(max_words_between)+r'} *\\b'\n",
    "        end   = r'\\b)'\n",
    "        \n",
    "    regex_list = []\n",
    "    \n",
    "    for permu in list(permutations(list_of_words)):\n",
    "        # catch this permutation: start + word + gap (+ word + gap)... + end\n",
    "        if cases_matter: # case sensitive - what cases the user gives are given back\n",
    "              regex_list.append(start+gap.join(permu)+end)           \n",
    "        else: # the resulting search will only work if all words are lowercase\n",
    "            lowerpermu = [w.lower() for w in permu]\n",
    "            regex_list.append(start+gap.join(lowerpermu)+end)\n",
    "    \n",
    "    return '|'.join(regex_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the word \"part\" near the word \"with\" in this string? No:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from near_regex import NEAR_regex\n",
    "import re\n",
    "\n",
    "test  = 'This is a partial string another break with words'\n",
    "words = ['part','with']\n",
    "rgx   = NEAR_regex(words)\n",
    "print(len(re.findall(rgx,test)))            # no match (partials not allowed by default) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can use `partial=True` in the function which will look for any words starting with \"part\" (partial, partly, etc) and \"with\" (without, withhold, etc). Be careful using `partial=True`, as it can lead to too many matches, and ones you don't want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "rgx = NEAR_regex(words,partial=True)\n",
    "print(len(re.findall(rgx,test)))            # match (partials allowed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change how many words can be between the search terms. Shorter distances means the terms are more likely to be related grammatically. For example, `max_words_between=3` might be used if you want to find an adjective directly modifying a noun (but with the allowance that it might be in a list of adjectives). Higher values are more useful if you want to see if two ideas (nouns) are in the same sentence or paragraph. \n",
    "\n",
    "In our example, \"partial\" and \"with\" are several words apart, so this correctly returns a value of zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "rgx   = NEAR_regex(words,partial=True,max_words_between=1)\n",
    "print(len(re.findall(rgx,test)))            # no match (too far apart) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Two caveats:\n",
    ":class: note\n",
    "1. This is a \"dumb\" function in the sense that it doesn't do anything to actually detect if two words are in the same sentence or paragraph. There are ways to do that, but we're going to keep it simple and just stick to this function as-is. Let's assume that a sentence is about 20 words on average, and a paragraph is 200 words on average.\n",
    "2. In the next two examples: If your document has line breaks, like a newline symbol (\"\\n\") or return symbol (\"\\r\"), which create paragraph breaks in a document, this this function _won't_ search both sides of it. If a document splits paragraphs like this, great: our function will do within-paragraph searching, **as long as our cleaning process doesn't delete those symbols.** However, some documents have those symbols at the end of every single line and we need to delete them while cleaning, in which case, we can't assume the function only looks within paragraphs.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test  = 'This is a partial string \\n another break with words'\n",
    "words = ['part','with']\n",
    "rgx = NEAR_regex(words,partial=True)\n",
    "print(len(re.findall(rgx,test)))            # fails because of the \\n break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test  = 'This is a partial string \\r another break with words'\n",
    "words = ['part','with']\n",
    "rgx = NEAR_regex(words,partial=True)\n",
    "print(len(re.findall(rgx,test)))            # fails with \\r too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test  = 'This is a partial string                      another break with words'\n",
    "words = ['part','with']\n",
    "rgx = NEAR_regex(words,partial=True)\n",
    "print(len(re.findall(rgx,test)))            # extra spaces are treated as one space, no impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cases_matter` parameter is pretty simple. If true, then it only reports a match if the document's case exactly match your search terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "words = ['part','With']                     # changed to Capitalized \"With\"\n",
    "rgx   = NEAR_regex(words,partial=True,cases_matter=True)\n",
    "print(len(re.findall(rgx,test)))            # no match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look for three terms that all have to be next to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "words = ['part','with','this']\n",
    "rgx = NEAR_regex(words,partial=True)\n",
    "print(len(re.findall(rgx,test)))            # no match - good! \"This\" != \"this\"\n",
    "print(len(re.findall(rgx,test.lower())))    # match - good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} **Is topic1 near topic2 (instead of word1 near word2)?**\n",
    ":class: tip\n",
    "\n",
    "**The function doesn't just look for words that are near each other. It looks to see if _regex patterns_ are near each other.**\n",
    "\n",
    "Above, I said that NEAR_regex builds \"a (complex) regex pattern that will detect if all of the **strings** are near each other in the document.\"  What it is actually doing is putting the strings into a regex. In our examples so far, the strings are always a word. But your list of search terms can include \"mini-regexs\". The most common use: You want to look for a topic (a word) but make sure it is being used in a specific way.\n",
    "```\n",
    "\n",
    "Suppose we want to know if a text message chain involves a greeting _specifically_ for James but not John? But annoyingly, James is sometimes referred to by a nickname!\n",
    "\n",
    "Well, we'd want to build a search that looks for:\n",
    "1. A greeting term: \"hey\" OR \"hi\" or \"sup\". \n",
    "2. James' name and nicknames: \"james\" OR \"jimmy\"\n",
    "\n",
    "So we can exploit how regex works so that when we specify the `words = [element1, element2]` list, that \n",
    "1. `element1` is the list of greeting terms.\n",
    "1. `element2` is the list of acceptable names.\n",
    "\n",
    "**To build a regex that looks for \"hey\" OR \"hi\" or \"sup\", we need to implement these three things:\n",
    "- In regex, OR is \"|\".\n",
    "- No spaces between terms\n",
    "- **Important:* Put the parentheses around the whole set of terms!\n",
    "\n",
    "So, \"hey\" OR \"hi\" or \"sup\" becomes `'(hey|hi|sup)'`. \n",
    "\n",
    "Constructing a search like this allows you to look for a topic (here, the greeting term) being used in a specific way. Examples:\n",
    "1. The cost (term 1) of building factories (term 2)... but not general costs or costs for other items\n",
    "2. Discussions of risks (term 1) from particular sources like floods (term 2)... but not general risks like competition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "texts  = 'hey jimmy                      hi james          sup john'\n",
    "words = ['(hey|hi|sup)','(jimmy|james)']     # search for a greeting \n",
    "rgx = NEAR_regex(words,max_words_between=1)\n",
    "print(len(re.findall(rgx,texts)))            # both are caught"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Printing out your matches\n",
    ":class: tip\n",
    "\n",
    "This is very useful to double-check what your search is finding!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey jimmy', 'hi james']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m.group(0) for m in re.finditer(rgx,texts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Caveats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Two caveats\n",
    ":class: warning\n",
    "\n",
    "**Caveat One:** When a regex finds a match, the part of that string in the match can't be used again and thus won't be counted. Above, you could argue that mechanically, there are FOUR matches:\n",
    "1. hey jimmy\n",
    "2. jimmy hi \n",
    "3. hi james\n",
    "4. james sup\n",
    "\n",
    "But the \"hey jimmy\" match \"uses up\" \"jimmy\", so \"jimmy hi\" can't be found. Same with matches \\#3 and \\#4. \n",
    "\n",
    "**Caveat Two:** The function is \"greedy\". Look at the next example, where I allow there to be two words between.\n",
    "\n",
    "Starting from \"hey\", it finds \"jimmy\" right away, but it doesn't stop. (That would be a \"lazy\" regex.) It keeps looking, since it can look up to three words away. It takes the largest mechanical match possible, so when it finds james, it deems \"hey jimmy hi james\" a match, and because of that, \"hi james\" is already used and can't be a match. So it reports 1 match, not 2. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hey jimmy                      hi james']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgx = NEAR_regex(words,max_words_between=2)\n",
    "print(len(re.findall(rgx,texts)))            # the regex isn't greedy - it misses inner matches!\n",
    "[m.group(0) for m in re.finditer(rgx,texts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Let's use Telsa. Copy this code into an empty notebook, and put the near_regex.py file in the same folder. \n",
    "\n",
    "Let's see how many times the document mentions China in the context of three issues:\n",
    "1. political considerations (broadly, and specific risks)\n",
    "1. rare earth elements \n",
    "1. supply chains, import tariffs\n",
    "\n",
    "I strongly suggest approaching this by using two terms.  You might need to have multiple words in each term. \n",
    "\n",
    "\n",
    "1. Print out the text of the hits!\n",
    "2. Play around with partial and the max parameters. Specifically look for when changing them gives you BAD results: Either way too many non-results are included, or your search to returns too few results because a single hit encompasses several hits that should be separately counted\n",
    "3. Try to get the \"most accurate\" sense of\n",
    "    1. Does the document discuss each issue at all? Is the number of hits 0, or >0?\n",
    "    1. How much does the document discuss the issue? How many hits?\n",
    "4. Does the number of hits reflect the scale of the issue? Why or why not?\n",
    "5. If not, is there a way to transform the number of hits to reflect the scale? Some ideas:\n",
    "    - Going from 1 to 2 hits is probably more informative that from 99 hits to 100\n",
    "    - If you use ln(hits), ln(2)-ln(1) is more than ln(100)-ln(99)\n",
    "    - Try to use ln(hits) and see if you spot a problem! What's a fix you can use?\n",
    "    - Binning: no discussion (hits=0), some discussion (hits<=#), lots of discussion (hits>#)\n",
    "    - How many bins and how you set the cutoffs is ad hoc and depends on the documents and topics!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'https://www.sec.gov/Archives/edgar/data/1318605/000156459019003165/0001564590-19-003165.txt'\n",
    "r = requests.get(url)\n",
    "\n",
    "# putting the \"Good ideas\" from 4.4 to work to clean the document:\n",
    "lower = BeautifulSoup(r.content).get_text().lower()\n",
    "no_punc = re.sub(r'\\W',' ',lower)\n",
    "cleaned = re.sub(r'\\s+',' ',no_punc).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the function into your working directory next to this file\n",
    "from near_regex import NEAR_regex \n",
    "# if this prints numbers, that means you havent deleted the examples inside near_regex.py. delete em\n",
    "\n",
    "help(NEAR_regex) # look at and read the documentation!\n",
    "\n",
    "# try to use NEAR_regex... look for it working and failing..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
